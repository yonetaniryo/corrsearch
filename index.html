<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>CorrSearch by yonetaniryo</title>

    <link rel="stylesheet" href="assets/stylesheets/styles.css">
    <link rel="stylesheet" href="assets/stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>CorrSearch</h1>
        <p>Yonetani, Kitani and Sato: &quot;Ego-Surfing First Person Videos&quot;, CVPR2015</p>

        <p class="view"><a href="https://github.com/yonetaniryo/corrsearch">View the Project on GitHub <small>yonetaniryo/corrsearch</small></a></p>


        <ul>
          <li><a href="https://github.com/yonetaniryo/corrsearch/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/yonetaniryo/corrsearch/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/yonetaniryo/corrsearch">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h2>
		<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>
		Searching for yourself on first person videos!</h2>

       <p>
	<img src="assets/corrsearch.png">
</p>
<p>We envision a future time when wearable cameras (e.g., small cameras in glasses or pinned on a shirt collar) are worn by the masses and record first-person point-of-view (POV) videos of everyday life. While these cameras can enable new assistive technologies and novel research challenges, they also raise serious privacy concerns. For example, first-person videos passively recorded by wearable cameras will necessarily include anyone who comes into the view of a camera -- with or without consent. Motivated by these benefits and risks, we develop a self-search technique tailored to first-person POV videos. The key observation of our work is that the egocentric head motion of a target person (i.e., the self) is observed both in the POV video of the target and observer. The motion correlation between the target person's video and the observer's video can then be used to uniquely identify instances of the self. We incorporate this feature into our proposed approach that computes the motion correlation over supervoxel hierarchies to localize target instances in observer videos. Our proposed approach significantly improves self-search performance over several well-known face detectors and recognizers. Furthermore, we show how our approach can enable several practical applications such as privacy filtering, automated video collection and social group discovery.</p>

<h3> Reference</h3>
<p>
Ryo Yonetani, Kris M. Kitani and Yoichi Sato, &quot;Ego-Surfing First Person Videos&quot;, CVPR2015.
<a href="assets/0935.pdf" target="_blank">preprint (7.5MB)</a> / <a href="assets/0935_ext.pdf" target="_blank">extended abstract (1.9MB)</a> 
</p>

<h3> Ego-Surf Dataset</h3>
<p>
This dataset contains syncrhonized first-person videos of a conversation scene recorded at 60fps for 30sec.
The following compressed files include:
<ul>
	<li>ppm/: input videos (sequences of input frames resized into 320x240*)</li>
	<li>sv/: supervoxel hierarchies computed by <a href="http://www.cse.buffalo.edu/~jcorso/r/supervoxels/" target="_blank">LIBSVX</a></li>
	<li>gt/: target masks (bounding boxes annoted at every 0.5sec)</li>
</ul>

<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/example.tar.gz">Example (79MB)</a><BR>
<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/I0.tar.gz">Indoor 0 (515MB)</a>, 
<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/I1.tar.gz">Indoor 1 (520MB)</a>, 
<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/I2.tar.gz">Indoor 2 (713MB)</a>, 
<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/I3.tar.gz">Indoor 3 (748MB)</a><BR> 
<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/O0.tar.gz">Outdoor 0 (609MB)</a>, 
<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/O1.tar.gz">Outdoor 1 (626MB)</a>, 
<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/O2.tar.gz">Outdoor 2 (908MB)</a>, 
<a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/data/EgoSurf/O3.tar.gz">Outdoor 3 (908MB)</a><BR>
*Please email to yonetani@iis.u-tokyo.ac.jp if you need videos at the original resolution (1920x1080).
</p>
<h3> CorrSearch Software (Python)</h3>
<p>
<a href="https://github.com/yonetaniryo/corrsearch">View this project on GitHub</a> to test our CorrSearch Software written in Python.
</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/yonetaniryo">yonetaniryo</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="assets/javascripts/scale.fix.js"></script>
    
  </body>
</html>
